from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import IntegerType, StringType, StructField, StructType, DateType

spark = SparkSession.builder.appName("HelloSpark").getOrCreate()
# book = spark.read.csv("c1.csv",header=True)
custSchema = StructType()\
                .add("Index", IntegerType())\
                .add("Customer Id", StringType())\
                .add("First Name", StringType())\
                .add("Last Name", StringType())\
                .add("Company", StringType())\
                .add("City", StringType())\
                .add("Country", StringType())\
                .add("Phone 1", StringType())\
                .add("Phone 2", StringType())\
                .add("Email", StringType())\
                .add("Subscription Date", DateType())\
                .add("Website", StringType())

book = spark.read.options(inferSchema='True')\
                .schema(custSchema)\
                .csv("c1.csv",header=True)



# for line in recordLn:
#     name=line["FName"]+" "+line["LName"]
#     subscrDt =line["Year"]
#     print("Name: ",name," Subscription Year: ",subscrDt)
book.printSchema()

# define schema 
# concat first and last name and create column called Name
# extract the year from subscription data in to Year column
# display all columns
#
# ToDo 
# write parquet files -- pending
# ODT
# Delta Lake
# Read multiple files (Ex: Orders from multiple files and join the records and list it)

recordLn=book.select(
    col("Index"),
    col("Customer Id"),
    concat_ws(' ',col("First Name"),col("Last Name")).alias("Name"),
    col("Company"),
    concat_ws(', ',col("City"),col("Country")).alias("Address"),
    col("Phone 1"),
    col("Phone 2"),
    col("Email"),
    col("Subscription Date"),
    year(col("Subscription Date")).alias("Year"),
    col("Website")
  ).show(n=30,vertical=False)


